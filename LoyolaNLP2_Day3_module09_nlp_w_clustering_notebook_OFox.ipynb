{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N papers= 87\n",
      " 3\n",
      "\n",
      "\n",
      "The Same Subject Continued\n",
      "\n",
      "(Concerning Dangers From Foreign Force and Influence)\n",
      "\n",
      "For the Independent Journal.\n",
      "\n",
      "\n",
      "\n",
      "JAY\n",
      "\n",
      "\n",
      "\n",
      "To the People of the State of New York:\n",
      "\n",
      "IT IS not a new observation that the people of any country (if,\n",
      "like the Americans, intelligent and wellinformed) seldom adopt and\n",
      "steadily persevere for many years in an erroneous opinion respecting\n",
      "their interests. That consideration naturally tends to create great\n",
      "respect for the high opinion which the people of America have so\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "with open('clustering_pg18.txt', 'r') as fin:\n",
    "    lines = fin.read()\n",
    "\n",
    "papers = re.split(r'FEDERALIST\\.? No\\.?', lines)\n",
    "\n",
    "# Sanity\n",
    "print(f'N papers= {len(papers)}')\n",
    "print(papers[3][0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Label the authors\n",
    "catAuthor = {'HAMILTON':0, 'JAY':0, 'MADISON':2}\n",
    "catAuthorLkup = {catAuthor[k]:k for k in catAuthor}\n",
    "\n",
    "# Prepare dataset for tf-idf\n",
    "Docs = papers[1:86]\n",
    "Authors = np.array([catAuthor[re.search(r'(HAMILTON|JAY|MADISON)', _).group(0)] for _ in Docs])\n",
    "\n",
    "# Sanity check\n",
    "print(Authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N data points= 85, M features= 8693\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_tfidf = TfidfVectorizer().fit_transform(Docs)\n",
    "\n",
    "print(f'N data points= {X_tfidf.shape[0]}, M features= {X_tfidf.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "Clusters = KMeans(n_clusters=2).fit_predict(X_tfidf.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Print the cluster IDs that each document is assigned to by kmeans\n",
    "print(Clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency: [0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "cm = np.argmax(contingency_matrix(Clusters, Authors), axis=1)\n",
    "\n",
    "print(f'Contingency: {cm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster id 0 matches with Authors 0 - JAY\n",
      "cluster id 1 matches with Authors 0 - JAY\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Mapping\n",
    "for cluster, author in enumerate(cm):\n",
    "    print(f'cluster id {cluster} matches with Authors {author} - {catAuthorLkup[author]}')\n",
    "\n",
    "# Map the Clusters\n",
    "Predicted = np.array([cm[c] for c in Clusters])\n",
    "print(Predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering accuracy= 0.824\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "acc = 0\n",
    "for i, c in enumerate(Clusters):\n",
    "    if cm[c] == Authors[i]:  # this part is tricky\n",
    "        acc += 1\n",
    "\n",
    "print(f'Clustering accuracy= {acc/len(Clusters):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand index= -0.060\n"
     ]
    }
   ],
   "source": [
    "# Rand index\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "print (f'Adjusted Rand index= {adjusted_rand_score(Authors, Clusters):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
